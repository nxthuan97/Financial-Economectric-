# This homework is written by Nguyen Xuan Thuan (412707007)

### ch08.06 (a)

#### Question: 
We are curious whether holding education, experience, and METRO constant, there is the same amount of random variation in wages for males and females. Suppose $\text{var}(e_i| x_i, \text{FEMALE} = 0) = \sigma^2_M$ and $\text{var}(e_i| x_i, \text{FEMALE} = 0) = \sigma^2_F$. We specifically wish to test the null hypothesis $\sigma^2_M=\sigma^2_F$ against $\sigma_M^2 ≠\sigma^2_F$. Using 577 observations on males, we obtain the sum of squared OLS residuals, $SSE_M = 97161.9174$. The regression using data on females yields $\hat{\sigma_F} = 12.024$. Test the nullhypothesis at the 5% level of significance. Clearly state the value of the test statistic and the rejection region, along with your conclusion.

#### Answer:

$df_M = N_M - K = 577 - 4 =573$

$df_F = N_F - K = (1000 - 577) - 4 =419$

$\hat{\sigma^2_M} = SSE_M/df_M = 97161.9174/573 = 169.567$

$F = \hat{\sigma^2_M}/\hat{\sigma^2_F} = 169.567/12.024^2 = 1.173$

$Flc = F_{(0.025,573,419)} = 0.8377$

$Fuc = F_{(0.975,573,419)} = 1.196781$

At the 5% level of significance, since the calculated F-statistic (1.173). The test statistic value falls in the non-rejection region. We cannot conclude that the random error variation is different for males and females.

```r
alpha <- 0.05
N <- 1000
K <- 4
N_M <- 577
N_F <- 1000 - 577
df_M <- N_M - K
df_F <- N_F - K
SSE_M <- 97161.9174
sig_F <- 12.024
sig_F_squared <- sig_F^2
sig_M_squared <- SSE_M / df_M
fstat <- sig_M_squared / sig_F_squared
Flc <- qf(alpha/2, df_M, df_F)
Fuc <- qf(1 - alpha/2, df_M, df_F)
cat(Flc, Fuc, fstat)
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.06a.png?raw=true)


### ch08.06 (b)

#### Question:
We hypothesize that married individuals, relying on spousal support, can seek wider employment types and hence holding all else equal should have more variable wages. Suppose
$\text{var}(e_i| x_i, \text{MARRIED} = 0) = \sigma^2_{SINGLE}$ and $\text{var}(e_i| x_i, \text{MARRIED} = 1) = \sigma^2_{MARRIED}$. Specify the null hypothesis $\sigma_{SINGLE}^2=\sigma^2_{MARRIED}$ versus the alternative hypothesis $\sigma^2_{MARRIED}>\sigma^2_{SINGLE}$. We add FEMALE to the wage equation as an explanatory variable, so that $$WAGE_i = \beta_1 + \beta_2 EDUC_i + \beta_3 EXPER_i + \beta_4 METRO_i +\beta_5 FEMALE+ e_i$$ Using $N = 400$ observations on single individuals, OLS estimation of the above equation yields a sum of squared residuals is 56231.0382. For the 600 married individuals, the sum of squared errors is 100,703.0471. Test the null hypothesis at the 5% level of significance. Clearly state the value of the test statistic and the rejection region, along with your conclusion.

#### Answer:

$\sigma^2_{SINGLE} = SSE_{SINGLE} / df_{SINGLE} = 56,231.0382 / 400-5 = 142.357$

$\sigma^2_{MARRIED} = SSE_{MARRIED} / df_{MARRIED} = 100,703.0471 / 600-5 = 169.249$

$F = \sigma^2_{MARRIED} / \sigma^2_{SINGLE} = 169.249/142.357 = 1.1889$

$Fc = F_{(0.95,595,395)} = 1.1647$

The test statistic value is 1.1889 larger than this critical value 1.1647, so we conclude that the random error variation is greater for married individuals than single individuals.

```r
alpha <- 0.05
SSE_S <- 56231.0382
df_S <- 400 - 5
SSE_M <- 100703.0471
df_M <- 600 - 5
sig_S_squared <- SSE_S / df_S
sig_M_squared <- SSE_M / df_M
fstat <- sig_M_squared / sig_S_squared
Fc <- qf(1-alpha, df_M, df_S)
cat(Fc, fstat)
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.06b.png?raw=true)

### ch08.06 (c)

#### Question:
Following the regression in part (b), we carry out the $NR^2$ test using the right-hand-side variables in (XR8.6b) as candidates related to the heteroskedasticity. The value of this statistic is 59.03. What do we conclude about heteroskedasticity, at the 5% level? Does this provide evidence about the issue discussed in part (b), whether the error variation is different for married and unmarried individuals? Explain.

#### Answer:

$NR^2 = 59.03$

$\chi^2_{(0.95,4)} = 9.488$

Base on above result $NR^2 > \chi^2_{(0.95,4)}$ , we reject the null hypothesis of homoskedasticity in the pooled regression.

```r
chisq_c <- qchisq(0.95, df = 4)
chisq_c
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.06c.png?raw=true)

### ch08.06 (d)

#### Question:

Following the regression in part (b) we carry out the White test for heteroskedasticity. The value of the test statistic is 78.82. What are the degrees of freedom of the test statistic? What is the  5% critical value for the test? What do you conclude?

$$ WAGE_i = \beta_1 + \beta_2 EDUC_i + \beta_3 EXPER_i + \beta_4 METRO_i + \beta_5 FEMALE_i + e_i $$

#### Answer:

df = number of variables + number of variables square + number of interactions - indicator variables =  4 + 4 + 6 - 2 = 12

Rejection region : $\chi^* > \chi_{(12,0.95)} = 21.02607  $

test statistic: $\chi^* = 78.82 $

$\chi^* = 78.82 > \chi_{(12,0.95)} = 21.02607$

$ Reject \ H_0 $

We have significant evidence to reject homoskedasticity. It means $ \sigma^2_{SINGLE} \neq \sigma^2_{MARRIED} $

```r
qchisq(0.95, df = 12)
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.06d.png?raw=true)

### ch08.06 (e)

#### Question:

The OLS fitted model from part (b), with usual and robust standard errors, is

$$\widehat{WAGE} = -17.77 + 2.50\ EDUC + 0.23\ EXPER + 3.23\ METRO - 4.20\ FEMALE$$

 | Variable |  intercept  |      EDUC     |     EXPER     |     METRO     |   FEMALE   |
 |:--------:|:-----------:|:-------------:|:-------------:|:-------------:|:----------:|
 |    se    |    2.36  *   |     0.14   *   |      0.031 *   |      1.05     |    0.81  *  |
 |  robust  |    2.50  *   |     0.16   *   |      0.029 *   |      0.84     |    0.80  *  |


For which coefficients have interval estimates gotten narrower? For which coefficients have interval estimates gotten wider? Is there an inconsistency in the results?

#### Answer:

Since robust standard error is bigger than usual standard errors for intercept and $EDUC$, the interval estimates for the intercept and the coefficient of $EDUC$ have gotten wider. Because robust standard error is smaller than usual standard errors for the rest variables, so the interval estimates for the others have gotten narrower. From the significance of indicator results, we can find that it is consistent between two standard errors.

### ch08.06 (f)

#### Question:

If we add $MARRIED$ to the model in part (b), we find that its $t-value$ using a White heteroskedasticity robust standard error is about 1.0. Does this conflict with, or is it compatible with, the result in (b) concerning heteroskedasticity? Explain.

#### Answer:

The addition of the intercept indicator variable $MARRIED$ allows the expected wage, conditional on the explanatory variables $EDUC$, $EXPER$, $METRO$, and $FEMALE$ to differ between those who are married and those who are not. 

In part (b), we were asking about the amount of error variation between the two groups (married & single), which we found to be significant.

So the above two issues are quite different.

---

### ch08.16 (a)

#### Question:

Use the data file $vacation$ to estimate the model by OLS. Construct a 95% interval estimate for the effect of one more child on miles traveled, holidng the two other variables constant.

#### Answer:

$b4 = -81.826$

$tc = 1.97214$

$se = 27.1296$

The 95% confidence intervals for $\beta_4$ is $b4 ± tc*se(b4) = -81.826 ± 1.972141 * 27.1296$

Using the t critical value of 1.97214, the estimate for the effect of one more child is -81.82642, se is 27.1296 and the 95% confidence interval estimate is  [ -135.3298 , -28.32302 ].

Therefore, we estimate that another child will reduce average miles traveled by between 28 and 135 miles, with 95% confidence.

```r
library(POE5Rdata)
data(vacation)
mile <- vacation$miles
income <- vacation$income
age <- vacation$age
kid <- vacation$kids
mod <- lm(mile~income+age+kid)
alpha <- 0.05
df <- 196
smod <- summary(mod)
hatb4 <- coef(smod)[4,1]
seb4 <- coef(smod)[4,2]
tc <- qt(1-alpha/2,df)
up <- hatb4+tc*seb4
lw <- hatb4-tc*seb4
cat("estimate of b4 is",hatb4,"\n")
tc
cat("[",lw,",",up,"]")
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.16a.png?raw=true)

### ch08.16 (b)

#### Question:

Plot the OLS residuals versus $INCOME$ and $AGE$. Do you observe any patterns suggesting that heteroskedasticity is present?

#### Answer:

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.16b1.png?raw=true)

In the plot of the residuals against income variation of the residuals increases as income increases

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.16b2.png?raw=true)

There is no apparent relationship between the magnitude of the residuals and age.

Thus, the graphs suggest that the error variance depends on income, but not age.

```r
plot(vacation$income, mod1[["residuals"]], xlab = 'income', ylab = 'residuals')
plot(vacation$age, mod1[["residuals"]], xlab = 'age', ylab = 'residuals')
```

### ch08.16 (c)

#### Question:

Sort the data accordingg to increasing magnitude of income. Estimate the model using the first 90 observations and again using the last 90 observations. Carry out the Goldfelf-Quandt  test for heteroskedasticity errors at the 5% level. State the null and alternative hypothesis.

#### Answer:

$H0: \sigma_1^2 = \sigma_2^2$

$H1: \sigma_1^2 > \sigma_2^2$

$\alpha = 0.05$

$df = n - k = 90-4=86$

$F^* = \hat{\sigma_2^2}/\hat{\sigma_1^2} = [315821.55/(90-4)] / [(101744.65)/(90-4)] = 3.104$

Rejection region : $\lbrace F : F^* > F_{(0.95,86,86)} = 1.4286 \rbrace $

$\because F^* = 3.104 > F_{(0.95,86,86)} = 1.4286 \therefore Reject \ H_0 $

We can conclude that the error variance depends on income.

```r
vacation_sort <- vacation[order(vacation$income), ]
vacation_sort1 <- head(vacation_sort, 90)
vacation_sort2 <- tail(vacation_sort, 90)
model_vacation_sort1 <- lm(miles ~ income + age + kids, data = vacation_sort1)
model_vacation_sort2 <- lm(miles ~ income + age + kids, data = vacation_sort2)
df1 <- model_vacation_sort1[["df.residual"]]
df2 <- model_vacation_sort2[["df.residual"]]
alpha <- 0.05
summary_model_vacation_sort1 <- summary(model_vacation_sort1)
summary_model_vacation_sort2 <- summary(model_vacation_sort2)
sigma1 <- summary_model_vacation_sort1[["sigma"]]
sigma2 <- summary_model_vacation_sort2[["sigma"]]
f_stat <- (sigma2^2 / df2) / (sigma1^2 / df1)
critical_value <- qf(1-alpha,df1,df2)
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.16c.png?raw=true)

### ch08.16 (d)

#### Question:

Estimate the model by OLS using heteroskedasticity robust standard errors. Construct a 95% interval estimate for the effect of one more child on miles traveled, holding the two other variables constant. How does this interval estimate compare to the one in (a)?

#### Answer

For (a), regular OLS method, the estimate for the effect of one more child is -81.82642, se is 27.1296 and the 95% confidence interval estimate is  [ -135.3298 , -28.32302 ].

Using heteroskedasticity robust standard errors, it has the same estimate for the effect of one more child, se is 29.15438 and the 95% interval estimate is [ -139.323 , -24.32986 ].
Compare to (a), its interval is wider since it has a larger standard error.

```r
cov1 <- hccm(mod,type = "hc1")
seb4.robust <- sqrt(cov1[4,4])
up <- hatb4+tc*seb4.robust
lw <- hatb4-tc*seb4.robust
cat("[",lw,",",up,"]")
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.16d.png?raw=true)

### ch08.16 (e)

#### Question:

Obtain GLS estimates assuming $σ^2= σ^2INCOME^2$. Using both conventional GLS and robust GLS standard errors, construct a 95% interval estimate for the effect of one more child on miles traveled,holding the two other variables constant. How do these interval estimates compare to the ones in (a) and (d)?

#### Answer:

Set the weight as $\frac{1}{\sqrt{x_i}}$ $=\frac{1}{\sqrt{INCOME}}$.  
Using GLS robust standard error, the estimate for the effect of one more child is -78.36334, se is 24.73552 and the 95% confidence interval estimate is  [ -127.1453 , -29.5814 ].

```r
w <- 1/income
mod.gls <- lm(mile~income+age+kid,weights = w)
smod.gls <- summary(mod.gls)
hatb4.gls <- coef(smod.gls)[4,1]
seb4.gls <- coef(smod.gls)[4,2]
up <- hatb4.gls+tc*seb4.gls
lw <- hatb4.gls-tc*seb4.gls
cat("GLS estimate of b4 is",hatb4.gls,"\n")
cat("[",lw,",",up,"]")
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.16e.png?raw=true)

Compare to (a) and (d). GLS method has the smallest standard error and narrowest 95% confidence interval.

|                        | OLS                      | Heteroskedasticity robust standard errors | GLS                      |
|------------------------|--------------------------|-------------------------------------------|--------------------------|
|  $b_4$                |  -81.82642                | -81.82642                                 | -78.36334                |
| $se(b_4)$             |27.1296                 | 29.15438                                   | 24.73552                 |
| 95% interval  | [ -135.3298 , -28.32302 ] | [ -139.323 , -24.32986 ]                  | [ -127.1453 , -29.5814 ] |

---

### ch08.28 (a)

#### Question:
Regress $Y$ on $X2$ and $X3$ and obtain conventional OLS standard errors. Compare the estimeted coefficients to the true values of the regression parameters, $\beta_1=5$, $\beta_2=4$, $\beta_3=0$. Do the $t$-values suggest that the coefficients are significantly different from 0 at the 5% level?

#### Answer:

|  | OLS standard error |
|:---:|:---:|
| intercept | 30.367 |
| X2 | 6.537 |
| X3 | 6.307 |

|  | Estimated coefficients | True values |
|:---:|:---:|:---:|
| $b_1$ / $\beta_1$ | -58.467 | 5 |
| $b_2$ / $\beta_2$ | 16.355 | 4 |
| $b_3$ / $\beta_3$ | 8.880 | 0 |

##### Test the coefficients are significantly different from zero on not.
$H_0 : b = \beta$ and the critical value = 1.984723  
For $\beta_1$ : $t$-value = -2.089986  
For $\beta_2$ : $t$-value = 1.890022  
For $\beta_3$ : $t$-value = 1.407854  
The result shows that $\beta_1$ is not significantly different from 5 at the 5% significance level, $\beta_2$ is not significantly different from 4 at the 5% significance level, and $\beta_3$ is not significantly different from 0 at the 5% significance level
  

```r
U1 <- runif(100, min = 0, max = 1)
U2 <- runif(100, min = 0, max = 1)
X2 <- 1 + 5 * U1
X3 <- 1 + 5 * U2
Z <- rnorm(100, mean = 0, sd = 1)
E <- exp(2 + 0.6 * X2) * Z
Y <- 5 + 4 * X2 + E
beta1 <- 5
beta2 <- 4
beta3 <- 0
alpha <- 0.05
data <- data.frame(Y = Y, X2 = X2, X3 = X3)
mod <- lm(Y ~ X2 + X3, data = data)
summary(mod)
coef_est <- coef(mod)
std_errors <- summary(mod)$coefficients[, "Std. Error"]
tc <- qt(1-0.05/2, mod$df.residual)
b1 <- coef(mod)[[1]]
b2 <- coef(mod)[[2]]
b3 <- coef(mod)[[3]]
seb1 <- sqrt(vcov(mod)[1,1])
seb2 <- sqrt(vcov(mod)[2,2])
seb3 <- sqrt(vcov(mod)[3,3])
test1 <- (b1-5)/seb1
test2 <- (b2-4)/seb2
test3 <- (b3-0)/seb3
cat("T value1:",test1,"T value2:",test2,"T value3:",test3)

plot(X2, resid(mod), main = "Residuals vs X2",
     xlab = "X2", ylab = "Residuals", pch = 19)
abline(h = 0, col = "blue")

plot($X3, resid(mod), main = "Residuals vs X3",
     xlab = "X3", ylab = "Residuals", pch = 19)
abline(h = 0, col = "red")
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28a1.png?raw=true)

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28a2.png?raw=true)

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28a3.png?raw=true)

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28a4.png?raw=true)

## ch08.28 (b)

#### Question:

Calculate the least squares residuals $\widehat{e}$ from the OLS estimation in (a) and regress $\widehat{e}^2$ on X2 and X3. What evidence, if any, do you find for the presence of heteroskedasticity?  

#### Answer:

The estimated regression is $\widehat{e}^2$ = -10949.7 + 5938.3X2 −216.6X3.

The regression $R^2$ = 0.2544 so that the BP test statistic $NR^2$ = 25.44 

Set $\alpha$=0.05. Since the $p$-value in the BP test is 0.0000004, conclude that heteroskedasticity may exist.

```r
resid <- residuals(mod)
resid_squared <- resid^2
resid_mod <- lm(resid_squared ~ X2 + X3)
N <- nobs(resid_mod)
gresid_mod <- glance(resid_mod)
S <- gresid_mod$df
Rsqres <- gresid_mod$r.squared
chisq <- N*Rsqres; chisq
pval <- 1-pchisq(chisq,S-1); pval
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28b1.png?raw=true)

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28b2.png?raw=true)

## ch08.28 (c)

#### Question:

Regress Y on X2 and X3 and obtain robust standard errors. Compare these to the conventional standard errors in (a).

#### Answer:

|  | OLS standard error | Robust standare error |
|:---:|:---:|:---:|
| intercept | 30.367200 | 25.413530 |
| X2 | 6.537053 | 8.364803 |
| X3 | 6.307136 | 5.652605 |

The robust standard errors are larger for constant term and X2 but not for the X3 .

```r
library(knitr)
library(lmtest)
library(sandwich)
kable(tidy(mod),caption= "Regular standard errors")
vcov(mod)

cov <- hccm(mod, type="hc3")
cov
HC <- coeftest(mod, vcov.=cov)
kable(tidy(HC),caption = "Robust (HC) standard errors")
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28c1.png?raw=true)

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28c2.png?raw=true)

## ch08.28 (d)

**Question:**

Assume the heteroskedasticity pattern is $\sigma^2X_2^2$. Obtain GLS estimates with conventional and robust standard errors. Are the GLS parameter estimates closer to the true parameter values or not? Which set of standard errors should be used?

#### Answer:

|     | OLS estimates |GLS estimates & Standard Errors| Robust GLS estimates & Standard Errors | 95% GLS CI| 95% Robust GLS CI |
|:---:|:-----------------------:|:-----------------------:|:--------------------------------:|:-----------------------:|:--------------------------------:|
|$b_1$|     -58.467     |       -21.846 (14.516)       |           -21.8462  (13.2346)         |  [ -50.65712 , 6.964679 ]        |      [ -48.11314 , 4.420707 ]        |
|$b_2$|     16.355       |     11.096 (4.803)       |           11.0958  (5.3876)         | [ 1.562777 , 20.62883 ]        |         [ 0.4029505 , 21.78866 ]           |
|$b_3$|     8.880       |    3.054 (3.398)       |           3.0538  (2.5294)         | [ -3.689508 , 9.797038 ]        |      [ -1.966321 , 8.07385 ]            |

Compared to OLS estimates, only $b_1$ is closer to the true parameter. Additionally, the robust standard errors are larger than the conventional ones, indicating a more conservative approach. Hence, robust GLS is preferred for the model.

```r
w <- 1/X2^2
gls <- lm(Y ~ X2 + X3, weights = w)
cov4 <- hccm(gls, type = "hc1")
vcvmod <- coeftest(gls, vcov. = cov4)
glsb1 <- coef(gls)[[1]]
glsb2 <- coef(gls)[[2]]
glsb3 <- coef(gls)[[3]]
glsseb1 <- sqrt(vcov(gls)[1,1])
glsrobseb1 <- sqrt(cov4[1,1])
glsseb2 <- sqrt(vcov(gls)[2,2])
glsrobseb2 <- sqrt(cov4[2,2])
glsseb3 <- sqrt(vcov(gls)[3,3])
glsrobseb3 <- sqrt(cov4[3,3])
lwgb1 <- glsb1-tc*glsseb1
ubgb1 <- glsb1+tc*glsseb1
lwgb2 <- glsb2-tc*glsseb2
ubgb2 <- glsb2+tc*glsseb2
lwgb3 <- glsb3-tc*glsseb3
ubgb3 <- glsb3+tc*glsseb3

lwgrb1 <- glsb1-tc*glsrobseb1
ubgrb1 <- glsb1+tc*glsrobseb1
lwgrb2 <- glsb2-tc*glsrobseb2
ubgrb2 <- glsb2+tc*glsrobseb2
lwgrb3 <- glsb3-tc*glsrobseb3
ubgrb3 <- glsb3+tc*glsrobseb3

cat("b1 GLS se:",glsseb1,",","b1 GLS Robust se:",glsrobseb1,"\n")
cat("b2 GLS se:",glsseb2,",","b2 GLS Robust se:",glsrobseb2,"\n")
cat("b3 GLS se:",glsseb3,",","b3 GLS Robust se:",glsrobseb3,"\n")
cat("b1 95% GLS CI: [",lwgb1,",",ubgb1,"]","\n")
cat("b2 95% GLS CI: [",lwgb2,",",ubgb2,"]","\n")
cat("b3 95% GLS CI: [",lwgb3,",",ubgb3,"]","\n")

cat("b1 95% robust GLS CI: [",lwgrb1,",",ubgrb1,"]","\n")
cat("b2 95% robust GLS CI: [",lwgrb2,",",ubgrb2,"]","\n")
cat("b2 95% robust GLS CI: [",lwgrb3,",",ubgrb3,"]","\n")
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28d.png?raw=true)

## ch08.28 (e)

#### Question:

Assume the multiplicative heteroskedasticity model $exp(α_1 + α_2X_2 + α_3X_3)$. Obtain FGLS estimates with conventional and robust standard errors. Are the FGLS estimates closer to the true parameter values than the GLS or OLS estimates? Which set of standard errors should be used?

#### Answer:

|     |FLGS Standard Errors|  FLGS Robust Standard Errors |
|:---:|:--------:|:-----------:|
|$b_1$| 12.13033 | 12.05339 |
|$b_2$|4.78481|  4.74994 |
|$b_3$|2.564982| 2.174326 |

The standard errors of FLGS are slightly larger than GLS and significantly smaller than OLS. Therefore, GLS estimates are closer to the true parameter values compared to FLGS. While the robust standard error of b2 in FLGS is slightly larger than that of GLS, the robust standard errors of other coefficients in FLGS are slightly smaller than GLS. Hence, robust FLGS is preferred for the model.

```r
mod <- lm(Y~X2+X3)
e_hatsq <- resid(mod)^2
sighatsq <- lm(log(e_hatsq)~X2+X3)
var <- exp(fitted(sighatsq))
flgs <- lm(Y~X2+X3,weights = 1/var)
cov5 <- hccm(flgs, type = "hc1")
flgsseb1 <- sqrt(vcov(flgs)[1,1])
flgsrobseb1 <- sqrt(cov5[1,1])
flgsseb2 <- sqrt(vcov(flgs)[2,2])
flgsrobseb2 <- sqrt(cov5[2,2])
flgsseb3 <- sqrt(vcov(flgs)[3,3])
flgsrobseb3 <- sqrt(cov5[3,3])
cat("b1 FLGS se:",flgsseb1,",","b1 FLGS Robust se:",flgsrobseb1,"\n")
cat("b2 FLGS se:",flgsseb2,",","b2 FLGS Robust se:",flgsrobseb2,"\n")
cat("b3 FLGS se:",flgsseb3,",","b3 FLGS Robust se:",flgsrobseb3,"\n")
```

![image](https://github.com/nxthuan97/Financial-Economectric-/blob/Rplots/Rplot08.28e.png?raw=true)

---